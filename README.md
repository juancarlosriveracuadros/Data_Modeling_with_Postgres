## Data Modelling with SQLPostgres
Data Modelling for the company Sparkify. The data is a collection of songs and user activities on their new music streaming app. The goal of the program is to understand what songs users are listening to. The information comes from two JSON directories. The program is a Postgres database with tables designed to optimize queries on song play analysis and an ETL pipeline created in python.

# JSON directories (Start Dataset)
Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.
columns: artist_id, artist_latitude, artist_location,artist_longitude, artist_name, duration, num_songs, song_id, title, year

Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator. These simulate activity logs from a music streaming app based on specified configurations.
columns: artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId

# Filter NextSong 
The NextSong filter is a condition for the data in the log Dataset. Only the values of page that are NextSong are taked into the tables. code in Python(panda) below.
df = df[df['page']=='NextSong'].reset_index()

# Tables data base in Posgres
Dimension Table
Data from Song Dataset (song_table, artist_table)
song_table: song_id, title, artist_id, year, duration
artist_table: artist_id, artist_name, artist_location, artist_latitude, artist_longitude

Data from Log Dataset (time_table, users) + NextSong
time_table:start_time, hour, day, week, month, year, weekday
users: userId, firstName, lastName, gender, level

Fact Table 
JOIN beteween Song Dataset und Log Dataset (songplay) + NextSong 
songplay:  songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

# Program description
1) All the files of Song Dataset are loaded in the program.
2) The tables song_table and artist_table are dropped, created and inserted with the information of the Song Dataset.
3) All the files of Log Dataset are loaded in the program.
4) The files are filtered with NextSong (columns page are equal to NextSong).
5) The time information of the filtered Song Dataset is created from the column ts and the table time_table is dropped, created and with the information inserted.
6) The table users is dropped, created, inserted with the filtered information of Log Dataset
7) A JOIN between Song Dataset and the filtered log Dataset (Song_select) take place to produce the information of songplay the songplay table is dropped, created and the information is inserted

# sql_queries.py
in sql_queries.py are the 4 kinds of basics queries and 2 queries lists: 

basic queries:
-Drop table
    songplay_table_drop
    user_table_drop
    song_table_drop
    artist_table_drop
    time_table_drop
-Create table
    songplay_table_create
    user_table_create
    song_table_create
    artist_table_create
    time_table_create
-Insert table
    songplay_table_insert
    user_table_insert
    song_table_insert
    artist_table_insert
    time_table_insert
-Song_select (JOIN between long Dataset and Song Dataset)

queries litsts:
-create_table_queries = songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create
-drop_table_queries = songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop

# create_tables.py
In the sql_queries.py are the lists queries (create_table_queries, drop_table_queries). There are the main queries for the table creation function. 
The function of table creation is located in create_tables.py. There are 3 basics functions create_database, drop_tables, create_tables:

function:
create_database: has 4 steps. 
    1) connect to the default database
    2) create sparkify database with UTF8 encoding
    3) close connection to the default database
    4) connect to sparkify database

drop_tables:
    Implementation of the queries list drop_table_queries in postgreSQL. the function needs conn.commit

create_tables: 
    Implementation of the queries list create_table_queries in postgreSQL. the function needs conn.commit

# etl.py = Extract, Transform and Load functions (main program)
in etl.py are three functions for the extraction, transformation, and load of the song_file and the log_file.

process_song_file:
    1) open song file
    2) insert song record
    3) insert artist record

process_log_file:
    1) open log file
    2) filter by NextSong action
    3) convert timestamp column to datetime
    4) insert time data records       
    5) load user table
    6) insert user records
    7) insert songplay records
    8) get songid and artistid from song and artist tables
    9) insert songplay record
    10) insert songplay record

process_data
    1) get all files matching extension from directory
    2) get the total number of files found
    3) iterate over files and process
